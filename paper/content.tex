%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

This is the introduction. Here's a text citation of \citet{OliveiraJr2015}
and a another version that is sometimes used \citep{OliveiraJr2015}.
The rest is gibberish text.

\lipsum[1]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methodology}

\subsection{Equivalent Source Technique}
\cite{Cordell1992}'s ‘generalized equivalent sources’ assumes any harmonic function $d_i$ can be approximated by the sum of $m$ discrete point source effects:
\begin{equation}
d_i = \sum_{j=1}^{m} c_j f_{ij}(x,y,z)
\end{equation}

These equations can also be expressed in matrix form as:
\begin{equation}
    \mathbf{d} = \mathbf{Ac}
\end{equation}
where $\mathbf{d}$ is the column vector of $n$ predicted values at the observation points, $\mathbf{c}$ is the column vector of $m$ coefficients (moment magnitude) and $\mathbf{A}$ is the $n \times m$ sensitivity (Jacobian) matrix. By minimising the goal function,
\begin{equation}
    \phi(c) = [\mathbf{d}^0 - \mathbf{Ac}]^T W[\mathbf{d}^0 - \mathbf{Ac}] + \lambda_d \mathbf{c}^T\mathbf{c}
\end{equation}
the values of the source coefficients, $c$, that best fit the observed field values can be obtained using the difference between the observed and the predicted data. If the difference is as close as possible to zero, the observed and predicted data are very similar. Therefore, the smallest,
\begin{equation}
    \phi(c) = min
\end{equation}
gives the best fit. Therefore, the derivetive of $\phi$ is equal to zero.
ADD MATHS!

\subsection{Gradient Boosting}
Estimating the source coefficients that best fit the observed data is computationally demanding. To overcome this problem, \cite{SolerUieda2021} used the gradient boosted method in which the source coefficients are estimated in overlapping windows and carried out iteratively. This gradient boosted method was applied to the shallow layer of equivalent sources.

\subsection{Norm of B Predictions}
The amplitude of the magnetic anomaly $B$ is weakly dependent on the direction of magnetisation compared the total field anomaly $F$ \cite{HidalgoGato2021}. By calculating the norm of the magnetic anomalous field, the data is less dependent on the direction of Earth’s main field and crustal magnetisation.

\subsection{Dual Layer Concept}
Two layers of equivalent sources was used to fit both, the regional magnetic field and the shallow magnetic anomalies. \cite{Li2019} found having an additional deeper layer of equivalent sources reduced the misfit, especially for the long‐wavelength fields. By using two layers, the deeper equivalent source layer can capture the regional, long-wavelength signals, whilst the shallower layer can capture the short-wavelength signals. For this dual layer process, firstly the deeper layer of equivalent sources is calculated to determine the regional field. To determine the depth of this layer, a lower and upper bound of 2.5 and 6 times the distance of the nearest neighbouring data point respectively was used \cite{Dampney1969}. The residual field between the deep equivalent source layer prediction and the data is then used to calculate the shallow equivalent source layer. Adding both the deep and shallow equivalent source layer predictions together creates the final prediction for the data.

\subsection{Block Averaging}
Block averaging the data reduces the computational load and the likelihood of overfitting the data. Due to the nature of aliasing flight line data, block averaging can balance the equivalent sources along and adjacent flight lines to reduce this effect \cite{SolerUieda2021}. Furthermore, by block averaging the data when calculating the deep equivalent sources, only the long-wavelength signals are captured, rather than both short and long wavelength signals. When the block average is too small, more short wavelength signals are captured and not all of the long wavelength signals from the regional field are captured. Too large block averaging, not all the regional signals are captured either. Therefore, different block averaging sizes were calculated to determine the optimum size for capturing only the long wavelength signals. To reduce the computational load when calculating the shallow equivalent sources, block averaging the same size as the grid spacing of the predicted data was applied.

\subsection{K-Fold Cross-Validation}
The model requires manual parameter selection for the damping, depth of equivalent sources and window size for the gradient boosting. In order to select the optimal parameter combination, K-Fold Crosss Validation (K-CV) was used. K-CV is a popular machine learning method often used for model selection. Data is divided into k number of folds, that are as equal as possible in size. One of the k folds is used as testing set and the remaining (k-1) folds are used as training sets. This is repeated iteratively until all k folds have been used as both testing and training sets. The K-CV error estimation is the root mean square error (RMSE) of all the errors from each fold calculation. The parameter combination with the smallest RMSE is selected for the model.

% KCV on slice data, plot pic of training and testing fold 

% https://ieeexplore.ieee.org/abstract/document/5342427
% https://academic.oup.com/biomet/article-abstract/76/3/503/298209?redirectedFrom=fulltext&login=true


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Synthetic Data Application}

\subsection{With and without dual layer}
* w/o dual layer - doesn't capture all of the regional signal

\subsection{Block average deep sources versus regular grids (for shallow we cite Santi):}
Block average the data to make it smaller and smoother. Show that this works.
Block average source positions instead of regular grid to avoid issues with no-data regions.

\subsection{Single synthetic model}
Use the coordinates from a survey (ICEGRAV) and make a dipole model that is relatively complex but doesn't have to be exactly like the data. Must have regional and shallow sources.


\subsection{Part 1}
Description of the model and data locations.

\subsection{Part 2}
Show the results for our method. Use the block averaging for deep sources + GB for shallow and predict a grid of TFA and |B|. Show maps of the block reduced sources and data, residuals after only deep sources, residuals of the TFA after gradient boosting, grid predictions (deep + shallow) of TFA and |B|.

\subsection{Part 3}
Difference between using the dual-layer or the single shallow layer. Maps showing the differences in TFA and |B|.

\subsection{Part 4}
Block averaging vs regular grid for deep sources. Effects on no-data zones when we use regular grid versus block averaged sources (maps showing the error). Difference in computation time between (1) Block averaged data and sources (2) Blocked averaged sources but original data (3) grid sources and original data (bar plot showing the difference in computation time).

\subsection{Brief discussion (or in a separate section)}

%\begin{figure}[tb]
%\centering
%\includegraphics[width=1\linewidth]{figures/simple-synthetic-data.png}
%\caption{
  %\lipsum[1]
%}
%\label{fig_synthetic_simple_data}
%\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Real Data Application}

\subsection{Apply the standard method to ICEGRAV}

\subsection{Describe ICEGRAV}

\subsection{Show the original data (TFA points)}

\subsection{Show the location of the deep sources and the block averaged data}

\subsection{Show residuals from deep sources}

\subsection{Show residuals from GB}

\subsection{Grid predictions of TFA and |B|}

\subsection{Brief discussion (or in a separate section)}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

\lipsum[1]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Open research}

The Python source code used to produce all results and figures presented here
is available at \url{https://github.com/\GitHubRepository} and
\url{https://doi.org/\ArchiveDOI} under the MIT open-source license.

Here we should cite all of the main software used, like Jupyter, numpy, scipy,
matplotlib, Fatiando, etc.

Cite any data sources as well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgements}

We are indebted to the developers and maintainers of the open-source software
without which this work would not have been possible.
Acknowledge any non-author contributors to this study.
Statement about funding.

% Thank the editors and reviewers after review.
